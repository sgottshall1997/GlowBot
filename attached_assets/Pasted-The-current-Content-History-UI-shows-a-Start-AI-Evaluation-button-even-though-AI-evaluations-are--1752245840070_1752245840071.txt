The current Content History UI shows a "Start AI Evaluation" button, even though AI evaluations are already completed during generation.

‚úÖ Please fix this behavior so that the content history card **automatically displays stored ChatGPT and Claude ratings + descriptions**, instead of prompting a manual evaluation.

Your task:

1. üîç **Fetch Stored Evaluation Data**
   - For each content item in the Content History:
     - Retrieve both `chatGPT` and `claude` evaluation results from the `contentEvaluations` table (or wherever they are stored).
     - Each rating should include:
       - Scores (e.g., virality, clarity, persuasiveness, creativity)
       - A short description or justification from each model

2. üß† **Update the ContentHistoryCard Component**
   - Replace the "Start AI Evaluation" button with a visual block that includes:
     - **ChatGPT Ratings**:
       - Average score (rounded)
       - Breakdown of each metric (1‚Äì10)
       - Short description (from GPT) like:  
         _"Clear and engaging with a strong hook, though could use more specific detail."_
     - **Claude Ratings**:
       - Average score
       - Metric breakdown
       - Justification like:  
         _"Well-structured and persuasive tone, but slightly generic in phrasing."_

3. üé® **Design/UI**
   - Display each model‚Äôs feedback in a separate card/section using the existing style system.
   - Use the brain icon or badges to distinguish ChatGPT and Claude clearly.
   - Add a tooltip or expandable view for the descriptions if space is tight.

4. üß™ **Test**
   - Test on both recent and older content pieces.
   - Ensure fallback logic works: if no evaluation exists (e.g. legacy data), the "Start AI Evaluation" button still shows.
   - Ensure no duplicate evaluation is triggered if already stored.

5. üßæ **Final Output**
   - Confirm that for all evaluated content:
     - Ratings are pulled from the DB and displayed in the UI
     - Button is hidden/replaced
     - All metrics and descriptions are shown cleanly

This should give the user instant access to both model evaluations right inside content history, with no manual trigger needed.